import json
from typing import TypedDict, Annotated, Sequence, List

import psycopg
from langchain_core.messages import (
    BaseMessage,
    HumanMessage,
    AIMessage,
    AIMessageChunk,
    ToolMessage,
)
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_deepseek import ChatDeepSeek
import os

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph import add_messages, StateGraph, START
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver

from config import config as app_config

from llm.llm_chat_with_tools.tools.calculate_tools import calculate_tools, advanced_math
from llm.llm_chat_with_tools.tools.search_tools import (
    search_tool,
    web_crawler,
)
from llm.llm_chat_with_tools.tools.result_processor import (
    result_processor,
    ProcessingMode,
)



# é…ç½®éªŒè¯
app_config.validate()


class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]


SimplePrompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ™ºèƒ½åŠ©æ‰‹ï¼Œæ‹¥æœ‰å¤šç§å·¥å…·èƒ½åŠ›ï¼Œè‡´åŠ›äºä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€åŠæ—¶ã€æœ‰ç”¨çš„ä¿¡æ¯å’Œè§£å†³æ–¹æ¡ˆã€‚

æ ¸å¿ƒèƒ½åŠ›ä¸å·¥å…·ï¼š
ğŸ” æ™ºèƒ½æœç´¢ï¼šå®æ—¶è·å–æœ€æ–°ç½‘ç»œä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–°é—»ã€èµ„è®¯ã€æŠ€æœ¯æ–‡æ¡£ç­‰
ğŸŒ ç½‘é¡µçˆ¬å–ï¼šæ·±åº¦è§£æç½‘é¡µå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å’Œè¯¦ç»†æ•°æ®  
ğŸ§® æ•°å­¦è®¡ç®—ï¼šå¤„ç†å„ç§æ•°å­¦è¿ç®—ã€ç»Ÿè®¡åˆ†æå’Œè®¡ç®—é—®é¢˜
ğŸ“Š æ•°æ®æŸ¥è¯¢ï¼šæŸ¥è¯¢æ•°æ®åº“ä¿¡æ¯ï¼Œå¦‚å­¦ç”Ÿæˆç»©ç»Ÿè®¡ç­‰

å·¥ä½œåŸåˆ™ï¼š
1. ä¿¡æ¯å‡†ç¡®æ€§ï¼šä¼˜å…ˆä½¿ç”¨æœç´¢å·¥å…·è·å–æœ€æ–°ã€çœŸå®çš„æ•°æ®ï¼Œé¿å…è¿‡æ—¶ä¿¡æ¯
2. æ¥æºæ ‡æ³¨ï¼šæ˜ç¡®æ ‡å‡ºä¿¡æ¯æ¥æºï¼Œæä¾›å¯éªŒè¯çš„å‚è€ƒé“¾æ¥
3. æ·±åº¦åˆ†æï¼šæœç´¢åæ ¹æ®éœ€è¦ä½¿ç”¨ç½‘é¡µçˆ¬å–å·¥å…·è·å–è¯¦ç»†å†…å®¹
4. ç»“æ„åŒ–å›ç­”ï¼šä»¥æ¸…æ™°ã€æœ‰æ¡ç†çš„æ–¹å¼ç»„ç»‡å’Œå‘ˆç°ä¿¡æ¯
5. ä¸»åŠ¨æ€è€ƒï¼šç†è§£ç”¨æˆ·çœŸå®æ„å›¾ï¼Œæä¾›è¶…å‡ºé¢„æœŸçš„æœ‰ä»·å€¼å»ºè®®

å“åº”ç­–ç•¥ï¼š
- å¯¹äºæ—¶æ•ˆæ€§å¼ºçš„é—®é¢˜ï¼ˆå¤©æ°”ã€æ–°é—»ã€è‚¡ä»·ç­‰ï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æœç´¢å·¥å…·
- å¯¹äºéœ€è¦è¯¦ç»†ä¿¡æ¯çš„æŸ¥è¯¢ï¼Œå…ˆæœç´¢æ¦‚å†µï¼Œå†çˆ¬å–å…·ä½“å†…å®¹
- å¯¹äºè®¡ç®—ç±»é—®é¢˜ï¼Œä½¿ç”¨è®¡ç®—å·¥å…·ç¡®ä¿å‡†ç¡®æ€§
- å¯¹äºæ•°æ®æŸ¥è¯¢éœ€æ±‚ï¼Œä½¿ç”¨ç›¸åº”çš„æŸ¥è¯¢å·¥å…·
- å§‹ç»ˆä»¥ç”¨æˆ·éœ€æ±‚ä¸ºå¯¼å‘ï¼Œçµæ´»è¿ç”¨å„ç§å·¥å…·ç»„åˆ

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜çš„æ€§è´¨ï¼Œæ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ç»„åˆæ¥æä¾›æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚"""

client = MultiServerMCPClient(
    {
        "Student_Grade_System": {
            "url": app_config.mcp_server_url,
            "transport": "streamable_http",
        }
    }
)

mcp_tools = []


async def get_tools():
    global mcp_tools
    mcp_tools = await client.get_tools()


class ChatBot:
    def __init__(
        self,
        model: str = "Qwen/Qwen2.5-7B-Instruct",
        enable_result_processing: bool = True,
    ):
        """
        ChatBotåˆå§‹åŒ–
        :param model: æ¨¡å‹åç§°
        :param enable_result_processing: æ˜¯å¦å¯ç”¨ç»“æœå¤„ç†
        """
        self.llm = ChatDeepSeek(
            model=model,
            verbose=False,
            temperature=0.6,
        )
        self.tools = [
            search_tool,
            calculate_tools,
            advanced_math,
            web_crawler,
        ]
        self.llm_with_tools = None
        self.tool_node = None
        self.enable_result_processing = enable_result_processing
        self.result_processor = result_processor

        self.prompt = ChatPromptTemplate.from_messages(
            [("system", SimplePrompt), ("placeholder", "{messages}")]
        )

        self.chain = None

        # self.graph = self.create_graph()
        self.memory = None
        self.graph = None

    async def initialize(self):
        conn = await psycopg.AsyncConnection.connect(
            app_config.database_url,
            autocommit=True,
        )
        self.memory = AsyncPostgresSaver(conn)

        await self.memory.setup()
        if not self.chain:
            await get_tools()
            self.tools.extend(mcp_tools)
            print(self.tools)
            self.llm_with_tools = self.llm.bind_tools(tools=self.tools)
            self.tool_node = ToolNode(tools=self.tools)

            self.chain = self.prompt | self.llm_with_tools
        self.graph = await self.create_graph()

    async def chatbot(self, state: ChatState):
        """
        chatbotèŠ‚ç‚¹
        :param state: langgraphçŠ¶æ€
        :return: æ–°langgraphçŠ¶æ€ï¼ŒåŒ…å«llm_out
        """
        messages = state["messages"]
        print(f"messages: {messages}")
        response = await self.chain.ainvoke({"messages": messages})
        return {"messages": response}

    async def process_tool_results(self, state: ChatState):
        """
        å¤„ç†å·¥å…·æ‰§è¡Œç»“æœçš„èŠ‚ç‚¹
        """
        messages = state["messages"]
        processed_messages = []

        for message in messages:
            if isinstance(message, ToolMessage) and self.enable_result_processing:
                # è·å–å·¥å…·åç§°
                tool_name = getattr(message, "name", "unknown_tool")
                original_content = message.content

                try:
                    # å¦‚æœæ˜¯MCPå·¥å…·ç»“æœï¼Œè¿›è¡Œå¤„ç†
                    if any(
                        tool_name.startswith(prefix)
                        for prefix in ["get_", "query_", "fetch_"]
                    ):
                        processed_content = await self.result_processor.process_result(
                            tool_name=tool_name,
                            result=original_content,
                            mode=ProcessingMode.FORMATTED,
                        )
                        message.content = processed_content
                except Exception as e:
                    print(f"å¤„ç†å·¥å…·ç»“æœæ—¶å‡ºé”™: {e}")
                    # ä¿æŒåŸå§‹å†…å®¹

            processed_messages.append(message)

        return {"messages": processed_messages}

    async def create_graph(self):
        if self.memory is None:
            raise ValueError("Memory not initialized. Call initialize() first.")

        graph_builder = StateGraph(ChatState)
        graph_builder.add_node("chatbot", self.chatbot, metadata={"name": "chatbot"})
        graph_builder.add_node("tools", self.tool_node, metadata={"name": "search"})

        if self.enable_result_processing:
            graph_builder.add_node(
                "process_results",
                self.process_tool_results,
                metadata={"name": "process_results"},
            )

        graph_builder.add_edge(START, "chatbot")
        graph_builder.add_conditional_edges("chatbot", tools_condition)

        if self.enable_result_processing:
            graph_builder.add_edge("tools", "process_results")
            graph_builder.add_edge("process_results", "chatbot")
        else:
            graph_builder.add_edge("tools", "chatbot")

        return graph_builder.compile(checkpointer=self.memory)

    async def generate(self, query: str, thread_id: str, summary_with_llm: bool = False):
        """
        å¯¹è¯å†…å®¹ç”Ÿæˆ
        :param query: é—®é¢˜å†…å®¹
        :param thread_id: çº¿ç¨‹id
        :param summary_with_llm: æ˜¯å¦å¯ç”¨LLMæ™ºèƒ½æ€»ç»“åŠŸèƒ½
        :return: streamè¿”å›llmå†…å®¹
        """
        if self.graph is None:
            await self.initialize()

        config: RunnableConfig = RunnableConfig(
            configurable={
                "thread_id": thread_id,
                "summary_with_llm": summary_with_llm
            }
        )
        full_messages = ""

        print("Start Chat:")
        async for chunk in self.graph.astream(
            {"messages": HumanMessage(content=query)},
            config=config,
            stream_mode="messages",
        ):
            # print(chunk)
            event = chunk[0]
            config = chunk[1]
            if isinstance(event, AIMessageChunk):
                if config.get("name") and config["name"] == "search":
                    continue
                full_messages += event.content
                print(event.content, end="")
            yield f"data: {json.dumps(dict(chunk[0]), ensure_ascii=False)}\n\n"
        # print(f"\nfull_messages:\n {full_messages}")
        yield "data: [DONE]\n"

    async def get_history(self, thread_id: str) -> List[BaseMessage]:
        """
        è·å–å†å²è®°å½•
        :param thread_id: å¯¹è¯çº¿ç¨‹ID
        :return: å†å²è®°å½•
        """
        if self.graph is None:
            await self.initialize()

        config: RunnableConfig = RunnableConfig(configurable={"thread_id": thread_id})
        try:
            state = await self.graph.aget_state(config)
            print(f"messages: {state.values['messages']}")
            if state and state.values and "messages" in state.values:
                return state.values["messages"]
            return []
        except Exception as e:
            print(f"Error getting history: {str(e)}")
            return []

    async def delete_history(self, thread_id: str) -> bool:
        """
        åˆ é™¤å†å²è®°å½•
        :param thread_id: çº¿ç¨‹ID
        :return: åˆ é™¤çŠ¶æ€
        """
        if self.graph is None:
            await self.initialize()

        try:
            await self.memory.adelete_thread(thread_id)
            return True
        except Exception as e:
            print(f"Error deleting history: {str(e)}")
            return False

    async def edit_message(
        self, thread_id: str, message_idx: int, new_content: str
    ) -> bool:
        """
        ç¼–è¾‘æ¶ˆæ¯
        :param thread_id: çº¿ç¨‹ID
        :param message_idx: æ¶ˆæ¯ä½ç½®
        :param new_content: æ–°å†…å®¹
        :return: bool ç¼–è¾‘çŠ¶æ€
        """
        if self.graph is None:
            await self.initialize()

        config = RunnableConfig(configurable={"thread_id": thread_id})
        try:
            state = await self.graph.aget_state(config)
            messages = state.values.get("messages", [])
            if 0 <= message_idx < len(messages):
                if isinstance(messages[message_idx], HumanMessage):
                    messages[message_idx] = HumanMessage(content=new_content)
                elif isinstance(messages[message_idx], AIMessage):
                    messages[message_idx] = AIMessage(content=new_content)
                else:
                    raise "Unsupported message type"
                await self.memory.adelete_thread(thread_id)
                await self.graph.aupdate_state(config, {"messages": messages})
                return True
            else:
                raise f"Message index {message_idx} out of range"
        except Exception as e:
            print(f"Error on edit message: {str(e)}")
            return False

    async def edit_message_with_id(
        self, thread_id: str, message_id: str, new_content: str
    ) -> bool:
        """
        æ ¹æ®æ¶ˆæ¯IDä¿®æ”¹æ¶ˆæ¯å†…å®¹
        :param thread_id: çº¿ç¨‹ID
        :param message_id: æ¶ˆæ¯ID
        :param new_content: æ–°æ¶ˆæ¯å†…å®¹
        :return: ä¿®æ”¹çŠ¶æ€
        """
        if self.graph is None:
            await self.initialize()

        config = RunnableConfig(configurable={"thread_id": thread_id})
        try:
            state = await self.graph.aget_state(config)
            messages = state.values.get("messages", [])
            idx = -1
            for i, message in enumerate(messages):
                if message.id == message_id:
                    idx = i
                    break
            if idx != -1:
                if isinstance(messages[idx], AIMessage):
                    messages[idx] = AIMessage(new_content)
                elif isinstance(messages[idx], HumanMessage):
                    messages[idx] = HumanMessage(new_content)
                else:
                    raise "Unsupported message type"

                await self.memory.adelete_thread(thread_id)
                await self.graph.aupdate_state(config, {"messages": messages})
                return True
            else:
                raise f"Unable to find messages index {message_id}"
        except Exception as e:
            print(f"Error on edit message with id: {str(e)}")
            return False

    async def delete_message(self, thread_id: str, message_idx: int) -> bool:
        """
        åˆ é™¤æŒ‡å®šæ¶ˆæ¯
        :param thread_id: çº¿ç¨‹ID
        :param message_idx: æ¶ˆæ¯ä½ç½®
        :return: åˆ é™¤çŠ¶æ€
        """
        if self.graph is None:
            await self.initialize()

        config = RunnableConfig(configurable={"thread_id": thread_id})
        try:
            state = await self.graph.aget_state(config)
            messages = state.values.get("messages", [])
            if 0 <= message_idx < len(messages):
                messages.pop(message_idx)
                await self.memory.adelete_thread(thread_id)
                await self.graph.aupdate_state(config, {"messages": messages})
                return True
            else:
                raise f"Message index out of range"
        except Exception as e:
            print(f"Error on delete message: {str(e)}")
            return False

    async def delete_message_with_id(self, thread_id: str, message_id: str) -> bool:
        """
        æ ¹æ®æ¶ˆæ¯IDåˆ é™¤æ¶ˆæ¯
        :param thread_id: çº¿ç¨‹ID
        :param message_id: æ¶ˆæ¯ID
        :return: åˆ é™¤çŠ¶æ€
        """
        if self.graph is None:
            await self.initialize()

        config = RunnableConfig(configurable={"thread_id": thread_id})
        try:
            state = await self.graph.aget_state(config)
            messages = state.values.get("messages", [])
            idx = -1
            for i, message in enumerate(messages):
                if message.id == message_id:
                    idx = i
                    break
            if idx != -1:
                messages.pop(idx)
            else:
                raise f"Message index {message_id} out of range"
            await self.memory.adelete_thread(thread_id)
            await self.graph.aupdate_state(config, {"messages": messages})
            return True
        except Exception as e:
            print(f"Error on delete message with id: {str(e)}")
            return False

    async def delete_messages_after_with_id(
        self, thread_id: str, message_id: str
    ) -> bool:
        """
        åˆ é™¤æŒ‡å®šæ¶ˆæ¯åé¢çš„æ‰€æœ‰æ¶ˆæ¯ï¼ŒåŒ…æ‹¬è¯¥æ¶ˆæ¯
        :param thread_id: çº¿ç¨‹ID
        :param message_id: æ¶ˆæ¯ID
        :return: åˆ é™¤æƒ…å†µ
        """
        if self.graph is None:
            await self.initialize()

        config = RunnableConfig(configurable={"thread_id": thread_id})
        try:
            state = await self.graph.aget_state(config)
            messages = state.values.get("messages", [])
            idx = -1
            for i, message in enumerate(messages):
                if message.id == message_id:
                    idx = i
                    break
            if idx != -1:
                if not (
                    isinstance(messages[idx], AIMessage)
                    or isinstance(messages[idx], HumanMessage)
                ):
                    raise "Unsupported message type"
                messages = messages[:idx]
                await self.memory.adelete_thread(thread_id)
                await self.graph.aupdate_state(config, {"messages": messages})
                return True
            else:
                raise f"Message index {message_id} out of range"
        except Exception as e:
            print(f"Error on delete message with id: {str(e)}")
            return False

    async def get_message_by_id(self, thread_id: str, message_id: str) -> BaseMessage:
        """
        æ ¹æ®æ¶ˆæ¯IDè·å–æŒ‡å®šæ¶ˆæ¯
        :param thread_id: çº¿ç¨‹ID
        :param message_id: æ¶ˆæ¯ID
        :return: æ¶ˆæ¯å¯¹è±¡ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        if self.graph is None:
            await self.initialize()

        config = RunnableConfig(configurable={"thread_id": thread_id})
        try:
            state = await self.graph.aget_state(config)
            messages = state.values.get("messages", [])
            for message in messages:
                if message.id == message_id:
                    return message
            return None
        except Exception as e:
            print(f"Error getting message by id: {str(e)}")
            return None

    async def generate_test(self, query: str):
        """
        æµ‹è¯•æ–¹æ³•ï¼ˆæµ‹è¯•ç”¨ï¼‰
        :param query: é—®é¢˜
        :return: None
        """
        config: RunnableConfig = RunnableConfig(configurable={"thread_id": "1"})
        full_messages = ""

        print("Start Chat: ")
        async for chunk in self.graph.astream(
            {"messages": HumanMessage(content=query)},
            config=config,
            stream_mode="messages",
        ):
            # print(chunk[0].content, end="")
            # print(chunk)
            data = chunk[1]
            print(data["name"])
            if isinstance(chunk[0], AIMessageChunk):
                full_messages += chunk[0].content

        print(f"full_messages: {full_messages}")


# async def main():
#     bot = ChatBot()
#     await bot.generate_test("æŸ¥è¯¢æ­å·çš„å¤©æ°”")
#
#
# if __name__ == "__main__":
#     asyncio.run(main())
