import asyncio
import json
from typing import List

import requests
from langchain.tools import tool
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.config import get_stream_writer
import os
from fastmcp import Client

search_url = "https://api.search1api.com/search"
crawl_url = "https://api.search1api.com/crawl"
search_api_key = "F016AD79-D8F4-4A2E-B53F-8578A4D43DDB"

client = Client("http://localhost:8080/mcp")


@tool
async def search_tool(query: str) -> str:
    """
    æœç´¢å¼•æ“å·¥å…·ï¼Œä»ç½‘ä¸Šæœç´¢å†…å®¹ï¼Œè¿”å›å†…å®¹åŒ…å«ï¼ˆç½‘é¡µæ ‡é¢˜ï¼Œç½‘é¡µç®€è¦å†…å®¹ï¼Œç½‘é¡µé“¾æ¥ï¼‰
    :param query: è¦æœç´¢çš„å†…å®¹
    :return: æœç´¢å¼•æ“ç»™å‡ºçš„ç»“æœ
    """
    # writer = get_stream_writer()
    # writer(f"web search: {query}")
    print(f"query: {query}")
    # await label_extra(query)
    response = requests.post(
        url=search_url,
        headers={
            "Authorization": f"Bearer {search_api_key}",
        },
        json={"query": query, "search_service": "google", "max_results": 10},
    )
    response.raise_for_status()
    response = response.json()

    result = ""
    for i, res in enumerate(response["results"]):
        result += str(i + 1) + " "
        result += "title: " + res["title"] + " "
        result += "snippet: " + res["snippet"] + " "
        result += "link: " + res["link"] + "\n"
        result += "\n"

    print(f"\nç½‘é¡µæœç´¢å†…å®¹ï¼š\n{result}")
    # writer(f"web search result: {result}")
    return result


@tool
def web_crawler(links: List[str]) -> str:
    """
    æ‰¹é‡ç½‘é¡µè®¿é—®å·¥å…·ï¼Œè·å–ç½‘é¡µè¯¦ç»†å†…å®¹å¹¶è¿›è¡Œç»“æ„åŒ–å¤„ç†
    :param links: é“¾æ¥å­—ç¬¦ä¸²æ•°ç»„
    :return: æ ¼å¼åŒ–åçš„ç½‘é¡µå†…å®¹ï¼ŒåŒ…å«æ ‡é¢˜ã€URLã€ä¸»è¦å†…å®¹å’Œå…³é”®ä¿¡æ¯
    """
    crawl_request = [{"url": link} for link in links]
    response = requests.post(
        url=crawl_url,
        headers={
            "Authorization": f"Bearer {search_api_key}",
        },
        json=crawl_request,
    )
    response.raise_for_status()
    response = response.json()
    print(f"\nç½‘é¡µçˆ¬å–åŸå§‹å†…å®¹:\n{response}")

    # å¤„ç†å’Œæ ¼å¼åŒ–ç½‘é¡µå†…å®¹
    formatted_content = format_crawled_content(response)
    print(f"\næ ¼å¼åŒ–åçš„ç½‘é¡µå†…å®¹:\n{formatted_content}")

    return formatted_content


def format_crawled_content(raw_response) -> str:
    """
    æ ¼å¼åŒ–çˆ¬å–çš„ç½‘é¡µå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ç»“æ„åŒ–å±•ç¤º
    :param raw_response: åŸå§‹APIå“åº”
    :return: ç»“æ„åŒ–çš„ç½‘é¡µå†…å®¹å­—ç¬¦ä¸²
    """
    if not raw_response or "results" not in raw_response:
        return "âŒ æ— æ³•è·å–ç½‘é¡µå†…å®¹"

    results = raw_response.get("results", [])
    if not results:
        return "âŒ ç½‘é¡µå†…å®¹ä¸ºç©º"

    formatted_pages = []

    for i, page_data in enumerate(results, 1):
        url = page_data.get("url", "æœªçŸ¥URL")
        title = page_data.get("title", "æ— æ ‡é¢˜").strip()
        content = page_data.get("content", "").strip()

        # å¤„ç†æ ‡é¢˜
        if not title or title == "æ— æ ‡é¢˜":
            title = extract_title_from_url(url)

        # å¤„ç†å†…å®¹
        if content:
            # æ¸…ç†å’Œæˆªå–å†…å®¹
            cleaned_content = clean_content(content)
            # æå–å…³é”®æ®µè½
            key_paragraphs = extract_key_paragraphs(cleaned_content)
            content_summary = key_paragraphs[:2000]  # é™åˆ¶é•¿åº¦
        else:
            content_summary = "æ— å¯ç”¨å†…å®¹"

        # æ ¼å¼åŒ–å•ä¸ªé¡µé¢å†…å®¹
        page_formatted = f"""
ğŸ“„ **ç½‘é¡µ {i}**: {title}
ğŸ”— **é“¾æ¥**: {url}
ğŸ“ **å†…å®¹æ‘˜è¦**:
{content_summary}
{'...' if len(content) > 2000 else ''}

---
"""
        formatted_pages.append(page_formatted)

    # ç»„åˆæ‰€æœ‰é¡µé¢å†…å®¹
    final_content = f"""
ğŸŒ **ç½‘é¡µçˆ¬å–ç»“æœ** (å…± {len(results)} ä¸ªé¡µé¢)

{''.join(formatted_pages)}

ğŸ’¡ **æç¤º**: ä»¥ä¸Šå†…å®¹æ¥è‡ªæŒ‡å®šç½‘é¡µçš„å®æ—¶çˆ¬å–ï¼Œä¿¡æ¯æ¥æºå·²æ ‡æ³¨ã€‚
"""

    return final_content.strip()


def extract_title_from_url(url: str) -> str:
    """ä»URLä¸­æå–å¯èƒ½çš„æ ‡é¢˜"""
    try:
        from urllib.parse import urlparse

        parsed = urlparse(url)
        domain = parsed.netloc.replace("www.", "")
        path_parts = [p for p in parsed.path.split("/") if p]
        if path_parts:
            return f"{domain} - {path_parts[-1]}"
        return domain
    except:
        return "æœªçŸ¥é¡µé¢"


def clean_content(content: str) -> str:
    """æ¸…ç†ç½‘é¡µå†…å®¹ï¼Œç§»é™¤å¤šä½™çš„ç©ºç™½å’Œæ— ç”¨å­—ç¬¦"""
    import re

    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    content = re.sub(r"\s+", " ", content)
    # ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
    content = re.sub(r"[\x00-\x1f\x7f-\x9f]", "", content)
    # ç§»é™¤å¤šä½™çš„æ¢è¡Œ
    content = re.sub(r"\n\s*\n", "\n\n", content)

    return content.strip()


def extract_key_paragraphs(content: str) -> str:
    """æå–å…³é”®æ®µè½ï¼Œä¼˜å…ˆæ˜¾ç¤ºè¾ƒé•¿çš„æ®µè½"""
    paragraphs = [p.strip() for p in content.split("\n") if p.strip()]

    # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½ï¼ˆå¯èƒ½æ˜¯å¯¼èˆªã€å¹¿å‘Šç­‰ï¼‰
    meaningful_paragraphs = [p for p in paragraphs if len(p) > 50]

    if not meaningful_paragraphs:
        meaningful_paragraphs = paragraphs[:5]  # å¦‚æœæ²¡æœ‰é•¿æ®µè½ï¼Œå–å‰5ä¸ª

    # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆå±•ç¤ºä¿¡æ¯é‡å¤§çš„æ®µè½
    meaningful_paragraphs.sort(key=len, reverse=True)

    # å–å‰å‡ ä¸ªæœ€æœ‰æ„ä¹‰çš„æ®µè½
    selected_paragraphs = meaningful_paragraphs[:3]

    return "\n\n".join(selected_paragraphs)


@tool
async def query_student_avg_grade(class_name: str) -> str:
    """
    æŸ¥è¯¢æŒ‡å®šç­çº§çš„å¹³å‡æˆç»©ç»Ÿè®¡ä¿¡æ¯ã€‚

    æ­¤å·¥å…·ç”¨äºè·å–ç‰¹å®šç­çº§æ‰€æœ‰å­¦ç”Ÿåœ¨è¯­æ–‡ã€æ•°å­¦ã€è‹±è¯­ä¸‰é—¨ç§‘ç›®çš„å¹³å‡åˆ†æ•°ï¼Œ
    ä»¥åŠè¯¥ç­çº§çš„å­¦ç”Ÿæ€»æ•°ã€‚é€‚ç”¨äºæ•™å¸ˆæŸ¥çœ‹ç­çº§æ•´ä½“å­¦ä¹ æƒ…å†µã€è¿›è¡Œæˆç»©åˆ†æç­‰åœºæ™¯ã€‚

    Args:
        class_name (str): è¦æŸ¥è¯¢çš„ç­çº§åç§°ï¼Œä¾‹å¦‚ "class_1"ã€"ä¸‰å¹´çº§ä¸€ç­" ç­‰ã€‚
                         ç­çº§åç§°éœ€è¦ä¸æ•°æ®åº“ä¸­å­˜å‚¨çš„å®Œå…¨åŒ¹é…ã€‚

    Returns:
        str: è¿”å›åŒ…å«ç­çº§å¹³å‡æˆç»©ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - class_name: ç­çº§åç§°
            - chinese_avg: è¯­æ–‡å¹³å‡åˆ†ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
            - math_avg: æ•°å­¦å¹³å‡åˆ†ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
            - english_avg: è‹±è¯­å¹³å‡åˆ†ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰

    Example:
        è¾“å…¥: "class_1"
        è¾“å‡º: {
            "class_name": "class_1",
            "chinese_avg": 85.67,
            "math_avg": 78.92,
            "english_avg": 82.45,
        }

    Usage Scenarios:
        - æ•™å¸ˆæŸ¥çœ‹ç­çº§æ•´ä½“å­¦ä¹ æ°´å¹³
        - å¯¹æ¯”ä¸åŒç­çº§çš„æˆç»©è¡¨ç°
        - ç”Ÿæˆç­çº§æˆç»©æŠ¥å‘Š
        - åˆ†æå„ç§‘ç›®å¼ºå¼±é¡¹

    Note:
        - å¦‚æœç­çº§ä¸å­˜åœ¨æˆ–æ²¡æœ‰æˆç»©æ•°æ®ï¼Œå°†è¿”å›ç›¸åº”çš„é”™è¯¯ä¿¡æ¯
        - å¹³å‡åˆ†è®¡ç®—åŸºäºè¯¥ç­çº§æ‰€æœ‰æœ‰æ•ˆçš„è€ƒè¯•æˆç»©è®°å½•
        - ç¡®ä¿è¾“å…¥çš„ç­çº§åç§°å‡†ç¡®æ— è¯¯
    """
    async with client:
        result = await client.call_tool(
            "get_class_average_grade", {"class_name": class_name}
        )
        json_str = result.content[0].text
        return json_str


async def label_extra(query):
    print(f"query2: {query}")
    llm = ChatOpenAI(
        model_name="Qwen/Qwen2.5-32B-Instruct",
        name="tool_llm",
        temperature=0.8,
        streaming=False,
        max_tokens=1024,
        base_url="https://api-inference.modelscope.cn/v1",
        api_key="a5a8fdf1-e914-4c1e-ac56-82888ec1be87",
    )
    config: RunnableConfig = {"configurable": {"thread_id": "tool_thread"}}
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ ä¸“é—¨ç”¨æ¥å¯¹å†…å®¹è¿›è¡Œæ ‡ç­¾æŠ½å– æ ‡ç­¾æŠ½å–æ–¹å¼æœ‰å…³é”®å­—æŠ½å–å’Œè¯­ä¹‰æŠ½å– ç‰¹åˆ«æ˜¯éšå«è¯­ä¹‰æŠ½å– ç„¶åè¿”å›æ ‡ç­¾åˆ—è¡¨\n"
                "è¦æ±‚ï¼š\n"
                "- ä¸¥æ ¼è¿”å› JSON æ•°ç»„æ ¼å¼"
                "- ç¦æ­¢è¾“å‡ºé™¤ JSON å¤–çš„ä»»ä½•æ–‡å­—\n"
                "- æ ‡ç­¾å¿…é¡»ç²¾ç‚¼å‡†ç¡®",
            ),
            ("human", "{input}"),
        ]
    )
    chain = prompt | llm | StrOutputParser()

    # res = await chain.ainvoke(input={"input": query})
    res = ""
    async for chunk in chain.astream(input={"input": query}, config=config):
        res += chunk
    print(f"label_extra: {type(res)}, {res}")
    res = json.loads(res)
    print(f"json res: {res}")
    return res


async def test():
    links = ["https://www.langchain.com/langgraph", "https://fastapi.tiangolo.com/"]
    web_crawler(links)
    # await label_extra("æ­å·å¤©æ°”")


if __name__ == "__main__":
    asyncio.run(test())
