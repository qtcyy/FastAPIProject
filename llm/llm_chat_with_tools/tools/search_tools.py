import json
from typing import List

import requests
from langchain.tools import tool
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
import os
from fastmcp import Client
from .result_processor import process_mcp_result

import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent.parent.parent))
from config import config as app_config

client = Client(app_config.mcp_server_url)


@tool
async def search_tool(query: str, config: RunnableConfig = None) -> str:
    """
    æ™ºèƒ½ç½‘ç»œæœç´¢å¼•æ“ - å®æ—¶è·å–æœ€æ–°ã€æœ€ç›¸å…³çš„ç½‘ç»œä¿¡æ¯

    åŠŸèƒ½ç‰¹ç‚¹ï¼š
    - åŸºäºGoogleæœç´¢å¼•æ“ï¼Œæä¾›é«˜è´¨é‡ã€å®æ—¶çš„æœç´¢ç»“æœ
    - æ™ºèƒ½åŒ¹é…ç”¨æˆ·æŸ¥è¯¢æ„å›¾ï¼Œè¿”å›æœ€ç›¸å…³çš„ç½‘é¡µä¿¡æ¯
    - ç»“æ„åŒ–å±•ç¤ºæœç´¢ç»“æœï¼ŒåŒ…å«æ ‡é¢˜ã€æ‘˜è¦å’Œé“¾æ¥
    - æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æŸ¥è¯¢ï¼Œè‡ªåŠ¨ä¼˜åŒ–æœç´¢ç­–ç•¥
    - è·å–æœ€å¤š10ä¸ªé«˜è´¨é‡æœç´¢ç»“æœ

    é€‚ç”¨åœºæ™¯ï¼š
    - è·å–æœ€æ–°æ–°é—»ã€èµ„è®¯å’Œäº‹ä»¶ä¿¡æ¯
    - æŸ¥æ‰¾äº§å“ä¿¡æ¯ã€ä»·æ ¼å¯¹æ¯”å’Œç”¨æˆ·è¯„ä»·
    - æœç´¢æŠ€æœ¯æ–‡æ¡£ã€æ•™ç¨‹å’Œè§£å†³æ–¹æ¡ˆ
    - è·å–å®æ—¶å¤©æ°”ã€è‚¡ä»·ã€æ±‡ç‡ç­‰åŠ¨æ€ä¿¡æ¯
    - æŸ¥æ‰¾å­¦æœ¯èµ„æ–™ã€ç»Ÿè®¡æ•°æ®å’Œç ”ç©¶æŠ¥å‘Š

    :param query: æœç´¢å…³é”®è¯æˆ–é—®é¢˜æè¿°ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢
    :return: æ ¼å¼åŒ–çš„æœç´¢ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«æ’åã€æ ‡é¢˜ã€å†…å®¹æ‘˜è¦å’Œæºç½‘é¡µé“¾æ¥

    ä½¿ç”¨å»ºè®®ï¼š
    - ä½¿ç”¨å…·ä½“ã€æ˜ç¡®çš„å…³é”®è¯ä»¥è·å¾—æ›´ç²¾å‡†çš„ç»“æœ
    - å¯ä»¥ç»„åˆå¤šä¸ªå…³é”®è¯æé«˜æœç´¢ç²¾åº¦
    - æœç´¢åå¯é…åˆweb_crawlerå·¥å…·è·å–è¯¦ç»†å†…å®¹
    """
    # writer = get_stream_writer()
    # writer(f"web search: {query}")
    print(f"query: {query}")
    # await label_extra(query)
    response = requests.post(
        url=app_config.search_api_url,
        headers={
            "Authorization": f"Bearer {app_config.search_api_key}",
        },
        json={"query": query, "search_service": "google", "max_results": 10},
    )
    response.raise_for_status()
    response = response.json()

    # æ ¼å¼åŒ–æœç´¢ç»“æœ
    formatted_result = format_search_results(response, query)

    print(f"\nç½‘é¡µæœç´¢å†…å®¹ï¼š\n{formatted_result}")
    # writer(f"web search result: {formatted_result}")

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨LLMæ€»ç»“åŠŸèƒ½
    summary_enabled = False
    if config and config.get("configurable"):
        summary_enabled = config["configurable"].get("summary_with_llm", False)

    print(f"ğŸ¤– LLMæ™ºèƒ½æ€»ç»“åŠŸèƒ½çŠ¶æ€: {'å¯ç”¨' if summary_enabled else 'å…³é—­'}")

    if summary_enabled:
        # llmå†æç‚¼
        print("ğŸ”„ æ­£åœ¨ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ€»ç»“...")
        answer = await summary_with_llm(formatted_result)
        return answer
    else:
        # ç›´æ¥è¿”å›æ ¼å¼åŒ–ç»“æœ
        print("ğŸ“„ è¿”å›åŸå§‹æ ¼å¼åŒ–æœç´¢ç»“æœ")
        return formatted_result


async def summary_with_llm(response: str) -> str:
    """
    ä½¿ç”¨LLMå¯¹æœç´¢ç»“æœè¿›è¡Œæ™ºèƒ½æ€»ç»“å’Œæç‚¼

    :param response: æ ¼å¼åŒ–çš„æœç´¢ç»“æœæ–‡æœ¬
    :return: LLMæ€»ç»“åçš„ç²¾ç‚¼å†…å®¹
    """
    try:
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æ€»ç»“åŠ©æ‰‹ã€‚è¯·å¯¹æä¾›çš„æœç´¢ç»“æœè¿›è¡Œæ™ºèƒ½æ€»ç»“å’Œæç‚¼ï¼š

1. æå–æœ€æ ¸å¿ƒã€æœ€æœ‰ä»·å€¼çš„ä¿¡æ¯
2. å»é™¤é‡å¤å’Œå†—ä½™å†…å®¹
3. æŒ‰é‡è¦æ€§æ’åºç»„ç»‡ä¿¡æ¯
4. ä¿æŒå®¢è§‚ä¸­æ€§ï¼Œä¸æ·»åŠ ä¸ªäººè§‚ç‚¹
5. ä½¿ç”¨æ¸…æ™°ç®€æ´çš„ä¸­æ–‡è¡¨è¾¾
6. å¦‚æœä¿¡æ¯æ¥æºå¤šæ ·ï¼Œè¯·æ³¨æ˜å…³é”®ä¿¡æ¯çš„æ¥æº

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨ç»“æ„åŒ–çš„markdownæ ¼å¼
- é‡è¦ä¿¡æ¯ç”¨ç²—ä½“æ ‡è®°
- é€‚å½“ä½¿ç”¨åˆ—è¡¨å’Œåˆ†æ®µ
- æ§åˆ¶åœ¨500å­—ä»¥å†…""",
                ),
                ("human", "è¯·æ€»ç»“ä»¥ä¸‹æœç´¢ç»“æœï¼š\n\n{search_content}"),
            ]
        )

        llm = ChatOpenAI(
            model="deepseek-ai/DeepSeek-V3",
            base_url=app_config.deepseek_api_base,
            api_key=app_config.deepseek_api_key,
            temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„è¾“å‡º
            max_tokens=1000,
        )

        chain = prompt | llm | StrOutputParser()
        print("summary_with_llm")
        summary_result = await chain.ainvoke({"search_content": response})

        # ç»„åˆæ€»ç»“å’ŒåŸå§‹é“¾æ¥ä¿¡æ¯
        combined_result = f"""## ğŸ¤– æ™ºèƒ½æ€»ç»“

{summary_result.strip()}

---

## ğŸ“š è¯¦ç»†æœç´¢ç»“æœ

{response}"""

        print(f"\n=== LLMæ€»ç»“ç»“æœ ===")
        print(f"åŸå§‹æœç´¢ç»“æœé•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"æ€»ç»“åå†…å®¹é•¿åº¦: {len(summary_result)} å­—ç¬¦")
        print(f"æ€»ç»“å†…å®¹:\n{summary_result}")
        print(f"=== æ€»ç»“ç»“æŸ ===\n")

        return combined_result

    except Exception as e:
        print(f"LLMæ€»ç»“å¤±è´¥: {e}")
        # å¦‚æœLLMæ€»ç»“å¤±è´¥ï¼Œè¿”å›åŸå§‹æ ¼å¼åŒ–ç»“æœ
        return response


async def summary_crawled_content(content: str) -> str:
    """
    ä½¿ç”¨LLMå¯¹ç½‘é¡µçˆ¬å–å†…å®¹è¿›è¡Œæ™ºèƒ½æ€»ç»“å’Œæç‚¼

    :param content: æ ¼å¼åŒ–çš„ç½‘é¡µçˆ¬å–å†…å®¹
    :return: LLMæ€»ç»“åçš„ç²¾ç‚¼å†…å®¹
    """
    try:
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µå†…å®¹æ€»ç»“åŠ©æ‰‹ã€‚è¯·å¯¹æä¾›çš„ç½‘é¡µçˆ¬å–å†…å®¹è¿›è¡Œæ™ºèƒ½æ€»ç»“å’Œåˆ†æï¼š

1. æå–æ¯ä¸ªç½‘é¡µçš„æ ¸å¿ƒä¿¡æ¯å’Œå…³é”®è¦ç‚¹
2. å»é™¤é‡å¤ã€å†—ä½™å’Œæ— å…³å†…å®¹
3. è¯†åˆ«ç½‘é¡µé—´çš„å…³è”æ€§å’Œäº’è¡¥æ€§
4. æŒ‰é‡è¦æ€§å’Œé€»è¾‘é¡ºåºç»„ç»‡ä¿¡æ¯
5. ä¿æŒå®¢è§‚ä¸­æ€§ï¼Œå‡†ç¡®ä¼ è¾¾åŸæ–‡æ„æ€
6. ä½¿ç”¨æ¸…æ™°ç®€æ´çš„ä¸­æ–‡è¡¨è¾¾
7. å¦‚æœå¤šä¸ªç½‘é¡µæ¶‰åŠåŒä¸€ä¸»é¢˜ï¼Œè¯·è¿›è¡Œç»¼åˆåˆ†æ

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨ç»“æ„åŒ–çš„markdownæ ¼å¼
- é‡è¦ä¿¡æ¯ç”¨ç²—ä½“æ ‡è®°
- é€‚å½“ä½¿ç”¨åˆ—è¡¨å’Œåˆ†æ®µ
- ä¸ºæ¯ä¸ªå…³é”®ä¿¡æ¯æ ‡æ³¨æ¥æºç½‘é¡µ
- æ§åˆ¶æ€»ç»“å†…å®¹åœ¨800å­—ä»¥å†…""",
                ),
                ("human", "è¯·æ€»ç»“åˆ†æä»¥ä¸‹ç½‘é¡µçˆ¬å–å†…å®¹ï¼š\n\n{crawled_content}"),
            ]
        )

        llm = ChatOpenAI(
            model="deepseek-ai/DeepSeek-V3",
            base_url=app_config.deepseek_api_base,
            api_key=app_config.deepseek_api_key,
            temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„è¾“å‡º
            max_tokens=1500,
        )

        chain = prompt | llm | StrOutputParser()
        summary_result = await chain.ainvoke({"crawled_content": content})

        # ç»„åˆæ€»ç»“å’ŒåŸå§‹å†…å®¹ä¿¡æ¯
        combined_result = f"""## ğŸ¤– æ™ºèƒ½æ€»ç»“åˆ†æ

{summary_result.strip()}

---

## ğŸ“š è¯¦ç»†ç½‘é¡µå†…å®¹

{content}"""

        print(f"\n=== ç½‘é¡µå†…å®¹LLMæ€»ç»“ç»“æœ ===")
        print(f"åŸå§‹ç½‘é¡µå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        print(f"æ€»ç»“åå†…å®¹é•¿åº¦: {len(summary_result)} å­—ç¬¦")
        print(f"æ€»ç»“å†…å®¹:\n{summary_result}")
        print(f"=== ç½‘é¡µæ€»ç»“ç»“æŸ ===\n")

        return combined_result

    except Exception as e:
        print(f"ç½‘é¡µå†…å®¹LLMæ€»ç»“å¤±è´¥: {e}")
        # å¦‚æœLLMæ€»ç»“å¤±è´¥ï¼Œè¿”å›åŸå§‹æ ¼å¼åŒ–ç»“æœ
        return content


def format_search_results(response, query: str) -> str:
    """
    æ ¼å¼åŒ–æœç´¢ç»“æœï¼Œæä¾›ç»“æ„åŒ–å’Œæ˜“è¯»çš„è¾“å‡º
    :param response: APIå“åº”ç»“æœ
    :param query: æœç´¢æŸ¥è¯¢
    :return: æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
    """
    if not response or "results" not in response:
        return "âŒ æœç´¢å¤±è´¥ï¼Œæœªèƒ½è·å–åˆ°ç»“æœ"

    results = response.get("results", [])
    if not results:
        return f"ğŸ” æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„æœç´¢ç»“æœ"

    # æ„å»ºæ ¼å¼åŒ–çš„æœç´¢ç»“æœ
    formatted_output = f"""
ğŸ” **æœç´¢æŸ¥è¯¢**: {query}
ğŸ“Š **æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ**

"""

    for i, res in enumerate(results, 1):
        title = res.get("title", "æ— æ ‡é¢˜").strip()
        snippet = res.get("snippet", "æ— æ‘˜è¦ä¿¡æ¯").strip()
        link = res.get("link", "")

        # æ¸…ç†å’Œä¼˜åŒ–æ‘˜è¦å†…å®¹
        snippet = clean_snippet(snippet)

        formatted_output += f"""ğŸ“„ **ç»“æœ {i}**: {title}
ğŸ”— **é“¾æ¥**: {link}
ğŸ“ **æ‘˜è¦**: {snippet}

---

"""

    # æ·»åŠ ä½¿ç”¨å»ºè®®
    formatted_output += f"""
ğŸ’¡ **æç¤º**: 
- ä»¥ä¸Šæ˜¯æœ€ç›¸å…³çš„ {len(results)} ä¸ªæœç´¢ç»“æœ
- ç‚¹å‡»é“¾æ¥å¯è®¿é—®åŸå§‹ç½‘é¡µè·å–å®Œæ•´ä¿¡æ¯
- å¦‚éœ€è·å–ç½‘é¡µè¯¦ç»†å†…å®¹ï¼Œå¯ä½¿ç”¨ web_crawler å·¥å…·
- æœç´¢ç»“æœæŒ‰ç›¸å…³æ€§æ’åºï¼Œæ’åè¶Šé å‰è¶Šç›¸å…³
"""

    return formatted_output.strip()


def clean_snippet(snippet: str) -> str:
    """
    æ¸…ç†æœç´¢ç»“æœæ‘˜è¦ï¼Œç§»é™¤å¤šä½™å­—ç¬¦å’Œæ ¼å¼åŒ–æ–‡æœ¬
    :param snippet: åŸå§‹æ‘˜è¦æ–‡æœ¬
    :return: æ¸…ç†åçš„æ‘˜è¦
    """
    import re

    if not snippet:
        return "æš‚æ— æ‘˜è¦ä¿¡æ¯"

    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    snippet = re.sub(r"\s+", " ", snippet)

    # ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
    snippet = re.sub(r"[^\x00-\x7F\u4e00-\u9fff\u3400-\u4dbf]", "", snippet)

    # é™åˆ¶é•¿åº¦ï¼Œé¿å…æ‘˜è¦è¿‡é•¿
    if len(snippet) > 200:
        snippet = snippet[:197] + "..."

    return snippet.strip()


@tool
async def web_crawler(links: List[str], config: RunnableConfig = None) -> str:
    """
    é«˜æ•ˆæ‰¹é‡ç½‘é¡µå†…å®¹æŠ“å–å·¥å…· - æ·±åº¦è§£æç½‘é¡µä¿¡æ¯å¹¶æ™ºèƒ½æå–å…³é”®å†…å®¹

    åŠŸèƒ½ç‰¹ç‚¹ï¼š
    - æ”¯æŒåŒæ—¶æŠ“å–å¤šä¸ªç½‘é¡µé“¾æ¥çš„å®Œæ•´å†…å®¹
    - è‡ªåŠ¨æå–ç½‘é¡µæ ‡é¢˜ã€æ­£æ–‡å†…å®¹å’Œå…³é”®ä¿¡æ¯æ®µè½
    - æ™ºèƒ½æ¸…ç†æ— å…³å†…å®¹ï¼ˆå¹¿å‘Šã€å¯¼èˆªç­‰ï¼‰ï¼Œä¸“æ³¨æ ¸å¿ƒä¿¡æ¯
    - ç»“æ„åŒ–è¾“å‡ºï¼Œä¾¿äºé˜…è¯»å’Œè¿›ä¸€æ­¥å¤„ç†
    - è‡ªåŠ¨å¤„ç†ç¼ºå¤±æ ‡é¢˜ï¼Œä»URLæ¨æ–­é¡µé¢ä¸»é¢˜
    - ğŸ¤– æ”¯æŒLLMæ™ºèƒ½æ€»ç»“ï¼šæ ¹æ®é…ç½®å¯å¼€å¯æ™ºèƒ½æ€»ç»“åŠŸèƒ½ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶å»é™¤å†—ä½™å†…å®¹

    ä½¿ç”¨åœºæ™¯ï¼š
    - è·å–æœç´¢ç»“æœé¡µé¢çš„è¯¦ç»†å†…å®¹
    - æ”¶é›†å¤šä¸ªç›¸å…³ç½‘é¡µçš„å®Œæ•´ä¿¡æ¯
    - æå–æ–‡ç« ã€æ–°é—»ã€äº§å“é¡µé¢çš„æ ¸å¿ƒå†…å®¹
    - å¯¹æ¯”åˆ†æä¸åŒç½‘ç«™çš„ä¿¡æ¯
    - é€šè¿‡LLMæ€»ç»“å¿«é€Ÿç†è§£å¤šä¸ªç½‘é¡µçš„æ ¸å¿ƒå†…å®¹

    :param links: éœ€è¦æŠ“å–çš„ç½‘é¡µURLåˆ—è¡¨ï¼Œæ”¯æŒHTTP/HTTPSåè®®
    :param config: è¿è¡Œé…ç½®ï¼ŒåŒ…å«summary_with_llmå‚æ•°æ§åˆ¶æ˜¯å¦å¯ç”¨LLMæ€»ç»“
    :return: ç»“æ„åŒ–çš„ç½‘é¡µå†…å®¹å­—ç¬¦ä¸²ï¼ŒåŒ…å«æ¯ä¸ªé¡µé¢çš„æ ‡é¢˜ã€é“¾æ¥ã€å†…å®¹æ‘˜è¦å’Œä¿¡æ¯æ¥æºæ ‡æ³¨ã€‚
            å¦‚å¯ç”¨LLMæ€»ç»“ï¼Œå°†è¿”å›æ™ºèƒ½æ€»ç»“åˆ†æ + åŸå§‹è¯¦ç»†å†…å®¹

    æ³¨æ„ï¼šå»ºè®®ä¸€æ¬¡æŠ“å–ä¸è¶…è¿‡5ä¸ªé“¾æ¥ï¼Œç¡®ä¿å“åº”é€Ÿåº¦å’Œå†…å®¹è´¨é‡
    """
    crawl_request = [{"url": link} for link in links]
    response = requests.post(
        url=app_config.crawl_api_url,
        headers={
            "Authorization": f"Bearer {app_config.search_api_key}",
        },
        json=crawl_request,
    )
    response.raise_for_status()
    response = response.json()
    print(f"\nç½‘é¡µçˆ¬å–åŸå§‹å†…å®¹:\n{response}")

    # å¤„ç†å’Œæ ¼å¼åŒ–ç½‘é¡µå†…å®¹
    formatted_content = format_crawled_content(response)
    print(f"\næ ¼å¼åŒ–åçš„ç½‘é¡µå†…å®¹:\n{formatted_content}")

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨LLMæ€»ç»“åŠŸèƒ½
    summary_enabled = False
    if config and config.get("configurable"):
        summary_enabled = config["configurable"].get("summary_with_llm", False)

    print(f"ğŸ¤– LLMæ™ºèƒ½æ€»ç»“åŠŸèƒ½çŠ¶æ€: {'å¯ç”¨' if summary_enabled else 'å…³é—­'}")

    if summary_enabled:
        # ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ€»ç»“
        print("ğŸ”„ æ­£åœ¨ä½¿ç”¨LLMå¯¹ç½‘é¡µå†…å®¹è¿›è¡Œæ™ºèƒ½æ€»ç»“...")
        summarized_content = await summary_crawled_content(formatted_content)
        return summarized_content
    else:
        # ç›´æ¥è¿”å›æ ¼å¼åŒ–ç»“æœ
        print("ğŸ“„ è¿”å›åŸå§‹æ ¼å¼åŒ–ç½‘é¡µå†…å®¹")
        return formatted_content


def format_crawled_content(raw_response) -> str:
    """
    æ ¼å¼åŒ–çˆ¬å–çš„ç½‘é¡µå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ç»“æ„åŒ–å±•ç¤º
    :param raw_response: åŸå§‹APIå“åº” (æ‰¹é‡çˆ¬å–æ ¼å¼)
    :return: ç»“æ„åŒ–çš„ç½‘é¡µå†…å®¹å­—ç¬¦ä¸²
    """
    if not raw_response:
        return "âŒ æ— æ³•è·å–ç½‘é¡µå†…å®¹"

    # å¤„ç†æ‰¹é‡çˆ¬å–å“åº”æ ¼å¼: [{"crawlParameters": {...}, "results": {...}}, ...]
    if isinstance(raw_response, list):
        crawl_results = raw_response
    else:
        # å…¼å®¹æ—§æ ¼å¼
        crawl_results = (
            raw_response.get("results", []) if "results" in raw_response else []
        )

    if not crawl_results:
        return "âŒ ç½‘é¡µå†…å®¹ä¸ºç©º"

    formatted_pages = []

    for i, crawl_item in enumerate(crawl_results, 1):
        # æ–°æ ¼å¼ï¼šä» crawl_item ä¸­æå– results
        if "results" in crawl_item:
            page_data = crawl_item["results"]
            crawl_params = crawl_item.get("crawlParameters", {})
            original_url = crawl_params.get("url", "æœªçŸ¥URL")
        else:
            # å…¼å®¹æ—§æ ¼å¼
            page_data = crawl_item
            original_url = page_data.get("url", "æœªçŸ¥URL")

        url = page_data.get("link", original_url)
        title = page_data.get("title", "æ— æ ‡é¢˜").strip()
        content = page_data.get("content", "").strip()

        # å¤„ç†æ ‡é¢˜
        if not title or title == "æ— æ ‡é¢˜":
            title = extract_title_from_url(url)

        # å¤„ç†å†…å®¹
        if content:
            # æ¸…ç†å’Œæˆªå–å†…å®¹
            cleaned_content = clean_content(content)
            # æå–å…³é”®æ®µè½
            key_paragraphs = extract_key_paragraphs(cleaned_content)
            content_summary = key_paragraphs[:2000]  # é™åˆ¶é•¿åº¦
        else:
            content_summary = "æ— å¯ç”¨å†…å®¹"

        # æ ¼å¼åŒ–å•ä¸ªé¡µé¢å†…å®¹
        page_formatted = f"""
ğŸ“„ **ç½‘é¡µ {i}**: {title}
ğŸ”— **é“¾æ¥**: {url}
ğŸ“ **å†…å®¹æ‘˜è¦**:
{content_summary}
{'...' if len(content) > 2000 else ''}

---
"""
        formatted_pages.append(page_formatted)

    # ç»„åˆæ‰€æœ‰é¡µé¢å†…å®¹
    final_content = f"""
ğŸŒ **ç½‘é¡µçˆ¬å–ç»“æœ** (å…± {len(crawl_results)} ä¸ªé¡µé¢)

{''.join(formatted_pages)}

ğŸ’¡ **æç¤º**: ä»¥ä¸Šå†…å®¹æ¥è‡ªæŒ‡å®šç½‘é¡µçš„å®æ—¶çˆ¬å–ï¼Œä¿¡æ¯æ¥æºå·²æ ‡æ³¨ã€‚
"""

    return final_content.strip()


def extract_title_from_url(url: str) -> str:
    """ä»URLä¸­æå–å¯èƒ½çš„æ ‡é¢˜"""
    try:
        from urllib.parse import urlparse

        parsed = urlparse(url)
        domain = parsed.netloc.replace("www.", "")
        path_parts = [p for p in parsed.path.split("/") if p]
        if path_parts:
            return f"{domain} - {path_parts[-1]}"
        return domain
    except:
        return "æœªçŸ¥é¡µé¢"


def clean_content(content: str) -> str:
    """æ¸…ç†ç½‘é¡µå†…å®¹ï¼Œç§»é™¤å¤šä½™çš„ç©ºç™½å’Œæ— ç”¨å­—ç¬¦"""
    import re

    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    content = re.sub(r"\s+", " ", content)
    # ç§»é™¤ç‰¹æ®Šæ§åˆ¶å­—ç¬¦
    content = re.sub(r"[\x00-\x1f\x7f-\x9f]", "", content)
    # ç§»é™¤å¤šä½™çš„æ¢è¡Œ
    content = re.sub(r"\n\s*\n", "\n\n", content)

    return content.strip()


def extract_key_paragraphs(content: str) -> str:
    """æå–å…³é”®æ®µè½ï¼Œä¼˜å…ˆæ˜¾ç¤ºè¾ƒé•¿çš„æ®µè½"""
    paragraphs = [p.strip() for p in content.split("\n") if p.strip()]

    # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½ï¼ˆå¯èƒ½æ˜¯å¯¼èˆªã€å¹¿å‘Šç­‰ï¼‰
    meaningful_paragraphs = [p for p in paragraphs if len(p) > 50]

    if not meaningful_paragraphs:
        meaningful_paragraphs = paragraphs[:5]  # å¦‚æœæ²¡æœ‰é•¿æ®µè½ï¼Œå–å‰5ä¸ª

    # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆå±•ç¤ºä¿¡æ¯é‡å¤§çš„æ®µè½
    meaningful_paragraphs.sort(key=len, reverse=True)

    # å–å‰å‡ ä¸ªæœ€æœ‰æ„ä¹‰çš„æ®µè½
    selected_paragraphs = meaningful_paragraphs[:3]

    return "\n\n".join(selected_paragraphs)


async def label_extra(query):
    print(f"query2: {query}")
    llm = ChatOpenAI(
        model_name="Qwen/Qwen2.5-32B-Instruct",
        name="tool_llm",
        temperature=0.8,
        streaming=False,
        max_tokens=1024,
        base_url="https://api-inference.modelscope.cn/v1",
        api_key="a5a8fdf1-e914-4c1e-ac56-82888ec1be87",
    )
    config: RunnableConfig = {"configurable": {"thread_id": "tool_thread"}}
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ ä¸“é—¨ç”¨æ¥å¯¹å†…å®¹è¿›è¡Œæ ‡ç­¾æŠ½å– æ ‡ç­¾æŠ½å–æ–¹å¼æœ‰å…³é”®å­—æŠ½å–å’Œè¯­ä¹‰æŠ½å– ç‰¹åˆ«æ˜¯éšå«è¯­ä¹‰æŠ½å– ç„¶åè¿”å›æ ‡ç­¾åˆ—è¡¨\n"
                "è¦æ±‚ï¼š\n"
                "- ä¸¥æ ¼è¿”å› JSON æ•°ç»„æ ¼å¼"
                "- ç¦æ­¢è¾“å‡ºé™¤ JSON å¤–çš„ä»»ä½•æ–‡å­—\n"
                "- æ ‡ç­¾å¿…é¡»ç²¾ç‚¼å‡†ç¡®",
            ),
            ("human", "{input}"),
        ]
    )
    chain = prompt | llm | StrOutputParser()

    # res = await chain.ainvoke(input={"input": query})
    res = ""
    async for chunk in chain.astream(input={"input": query}, config=config):
        res += chunk
    print(f"label_extra: {type(res)}, {res}")
    res = json.loads(res)
    print(f"json res: {res}")
    return res


# async def test():
#     links = ["https://www.langchain.com/langgraph", "https://fastapi.tiangolo.com/"]
#     web_crawler(links)
#     # await label_extra("æ­å·å¤©æ°”")
#
#
# if __name__ == "__main__":
#     asyncio.run(test())
